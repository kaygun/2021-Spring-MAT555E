# Description of the course

This course is designed to provide necessary mathematical and computational background on
statistical data analysis tools and algorithms needed by graduate students working in
applied sciences.  We are going to start by going over standard statistical data analysis
methods such as statistical tests and regression methods.  Then we are going to move on to
machine learning methods covering some stardard supervised and unsupervised machine learning
algorithms.  Along the way, we are going to need study various mathematical tools from
linear algebra and optimization theory.

# Technical requirements

The course is an applied data analysis class, and your performance is going to be judged
from 6 homeworks, and 1 final project.  This means the course requires a degree of
proficiency of computational tools from which you are going to be responsible.

Each student is going to be asked to open an [GitHub][1] account.  You will submit your
homeworks as GitHub projects that I am going to pull from GitHub at 11:59PM on each
deadline.  The homeworks are going to be [jupyter][2] notebooks written in [python][3]
language. You will need to install these tools on your local computational setup and learn
to work with these tools on your own.

[1]: https://github.com
[2]: https://jupyter.org
[3]: https://python.org

# Resources

## Books 

+ Ethem AlpaydÄ±n, "*Introduction to Machine Learning*." MIT Press.
+ Stuart J. Russell and Peter Norvig, "*Artificial Intelligence: A Modern Approach*." Prentice Hall.
+ Trevor Hastie, Robert Tibshirani, Jerome H. Friedman, "*Elements of Statistical Learning.*" Springer.
+ Kevin P. Murphy, "*Machine Learning: A Probabilistic Perspective*." MIT Press.
+ Jake VanderPlas, "*Python Data Science Handbook*." 

  Available on GitHub: https://github.com/jakevdp/PythonDataScienceHandbook

## Computational tools

* GitHub: https://github.com
* Jupyter notebooks: https://jupyter.org
* Python programming language: https://python.org
* Anaconda python environment: https://www.anaconda.com/products/individual
* Google colab: https://colab.research.google.com
* Microsoft azure notebooks: https://notebooks.azure.com/

## Some open data resources

* UCI datasets: https://archive.ics.uci.edu/ml/datasets.php
* Google dataset explorer: https://www.google.com/publicdata/directory
* Registry of open datasets on AWS: https://registry.opendata.aws/
* Open MRI, MEG, EEG, iEEG, and ECoG data: https://openneuro.org/
* NCBI datasets: https://www.ncbi.nlm.nih.gov/datasets/
* Open GIS data: https://www.usgs.gov/products/data-and-tools/gis-data

# Assessments

I am going to assign 6 homeworks.  For each homework you are going to have 1 week to
complete. Depending on your performance, I may choose several homeworks in each turn and ask
oral presentations on the howeworks handed in.  For the final project, I am going to talk
with each student and we will determine a distinct project to complete depending on your
interests.  Again, depending on your performance, I might also ask you to give an oral
presentation on your final project.

| **Homework**  | **Deadline** | **Percentage** |
|---------------|--------------|----------------|
| Homework 1    | 19 March     | 12%            |
| Homework 2    | 2 April      | 12%            |
| Homework 3    | 16 April     | 12%            |
| Homework 4    | 20 April     | 12%            |
| Homework 5    | 14   May     | 12%            |
| Homework 6    | 28   May     | 12%            |
| Final Project | TBA          | 28%            |

# Weekly Course Plan

1. Data science, machine learning, statistics and computer science. Connections,
   similarities, differences and interactions.
2. A crash course in computational tools: python and ecosystem of machine learning tools.
3. Supervised vs unsupervised learning models. Models and tests. Hypothesis
   testing. Statistical tests. Cross-validation.
4. Cost functions, distance functions, similarity measures. Optimization and regularization.
5. Least square regression. $R^2$, ANOVA, AIC and BIC. Regularized regressions, ridge and
   lasso regression.
6. Logistic and multinomial regression.
7. Linear algebraic methods: PCA, LDA and SVM.
8. Clustering and classification: K-means vs. K-NN.
9. Hierarchical clustering. Density based clustering.
10. Entropy and Gini coefficient. Decision trees and random forests.
11. Perceptron, graph computation and neural networks.
12. Hopfield networks, Boltzmann and restricted Boltzmann machines.
13. Examples and applications.
14. Examples and applications.
